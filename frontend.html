<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Assistant</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f7;
            color: #333;
        }
        .container {
            background: white;
            padding: 30px;
            border-radius: 12px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }
        h1 { text-align: center; margin-bottom: 30px; }
        .controls {
            display: flex;
            justify-content: center;
            align-items: center;
            gap: 20px;
            margin-bottom: 30px;
            flex-wrap: wrap;
        }
        button {
            padding: 12px 24px;
            font-size: 16px;
            border: none;
            border-radius: 24px;
            cursor: pointer;
            transition: transform 0.1s, opacity 0.2s;
            font-weight: 600;
        }
        button:active { transform: scale(0.98); }
        #startBtn { background-color: #007aff; color: white; }
        #stopBtn { background-color: #ff3b30; color: white; }
        button:disabled { opacity: 0.5; cursor: not-allowed; }

        /* Ambient Mode Switch */
        .switch-container {
            display: flex;
            align-items: center;
            gap: 10px;
            background: #f0f0f0;
            padding: 8px 16px;
            border-radius: 24px;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 28px;
        }
        .switch input { opacity: 0; width: 0; height: 0; }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0; left: 0; right: 0; bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 34px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 20px;
            width: 20px;
            left: 4px;
            bottom: 4px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider { background-color: #34c759; }
        input:checked + .slider:before { transform: translateX(22px); }
        .switch-label { font-weight: 600; font-size: 14px; }
        
        .status {
            text-align: center;
            margin-bottom: 20px;
            font-style: italic;
            color: #666;
            height: 24px;
        }
        
        .chat-box {
            border: 1px solid #e1e1e1;
            border-radius: 8px;
            height: 400px;
            overflow-y: auto;
            padding: 20px;
            background: #fafafa;
            display: flex;
            flex-direction: column;
            gap: 15px;
        }
        
        .message {
            max-width: 80%;
            padding: 10px 15px;
            border-radius: 18px;
            line-height: 1.4;
        }
        
        .user-message {
            align-self: flex-end;
            background-color: #007aff;
            color: white;
            border-bottom-right-radius: 4px;
        }
        
        .assistant-message {
            align-self: flex-start;
            background-color: #e5e5ea;
            color: black;
            border-bottom-left-radius: 4px;
        }
        
        .partial {
            opacity: 0.6;
            font-style: italic;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Voice Assistant</h1>
        
        <div class="status" id="status">Ready</div>
        
        <div class="controls">
            <button id="startBtn" onclick="startRecording()">Start Recording</button>
            <button id="stopBtn" onclick="stopRecording()" disabled>Stop Recording</button>
            
            <div class="switch-container">
                <span class="switch-label">Ambient Mode</span>
                <label class="switch">
                    <input type="checkbox" id="ambientToggle" onchange="toggleAmbientMode()">
                    <span class="slider"></span>
                </label>
            </div>
        </div>

        <div class="chat-box" id="chatBox">
            <div class="message assistant-message">Hello! I'm listening. How can I help you?</div>
        </div>
    </div>

    <script>
        // Configuration
        const WS_URL = "ws://localhost:8000/ws";
        
        // Global State
        let ws = null;
        let audioContext = null;
        let recognition = null;
        let isRecording = false; // "Mic Active" flag (HW level)

        // 1. CONVERSATION STATE MACHINE
        const AppState = {
            IDLE: 'idle',
            LISTENING: 'listening',
            PROCESSING: 'processing',
            SPEAKING: 'speaking'
        };
        let currentState = AppState.IDLE;
        let isAmbientMode = false;

        // Echo Gating
        let lastAgentResponse = ""; 

        // Audio Playback Queue
        let nextAudioTime = 0;
        let isAudioContextUnlocked = false;
        let activeSources = [];

        function ensureAudioContext() {
            if (!isAudioContextUnlocked || !audioContext) {
                 const AudioContext = window.AudioContext || window.webkitAudioContext;
                 if (!AudioContext) {
                     alert("Web Audio API is not supported in this browser");
                     return;
                 }
                 
                 if (!audioContext) {
                     audioContext = new AudioContext({ sampleRate: 24000 });
                 }
                 
                 if (audioContext.state === 'suspended') {
                     audioContext.resume();
                 }
                 
                 isAudioContextUnlocked = true;
                 nextAudioTime = audioContext.currentTime;
                 console.log("AudioContext unlocked/resumed");
            }
        }

        // UI Elements
        const statusEl = document.getElementById('status');
        const chatBox = document.getElementById('chatBox');
        const startBtn = document.getElementById('startBtn');
        const stopBtn = document.getElementById('stopBtn');
        const ambientToggle = document.getElementById('ambientToggle');

        // State Management
        function transitionTo(newState) {
            console.log(`State Transition: ${currentState} -> ${newState}`);
            currentState = newState;
            
            // UI Updates
            switch (newState) {
                case AppState.IDLE:
                    statusEl.textContent = "Ready";
                    startBtn.disabled = false;
                    stopBtn.disabled = true;
                    // Ensure mic is off
                    stopMic();
                    break;
                    
                case AppState.LISTENING:
                    statusEl.textContent = "Listening...";
                    startBtn.disabled = true;
                    stopBtn.disabled = false;
                    // Ensure mic is on
                    startMic();
                    break;
                    
                case AppState.PROCESSING:
                    statusEl.textContent = "Processing...";
                    // Mic stays on for ambient continuity, but we ignore input logically
                    // Or we stop it?
                    // Rule 4: "LISTENING -> PROCESSING: after user silence detected"
                    // If we want seamless, we keep mic on but ignore until ready?
                    // Actually, for Ambient, we usually keep it on.
                    break;
                    
                case AppState.SPEAKING:
                    statusEl.textContent = "Speaking...";
                    // Rule 3: "Immediately disable or pause STT processing"
                    // We DO NOT stop the mic (to allow barge-in detection), 
                    // but we strictly GATE the input in onresult.
                    break;
            }
        }

        function toggleAmbientMode() {
            // Ensure audio is ready when user clicks toggle
            ensureAudioContext();
            
            isAmbientMode = ambientToggle.checked;
            console.log("Ambient Mode:", isAmbientMode);
            
            if (isAmbientMode) {
                if (currentState === AppState.IDLE) {
                    transitionTo(AppState.LISTENING);
                }
            } else {
                // If turning off, go IDLE unless processing
                if (currentState === AppState.LISTENING) {
                    transitionTo(AppState.IDLE);
                }
                // Reset backend context
                if (ws && ws.readyState === WebSocket.OPEN) {
                    ws.send(JSON.stringify({ type: "reset_context" }));
                }
            }
        }

        // Initialize WebSocket
        function connectWebSocket() {
            ws = new WebSocket(WS_URL);
            
            ws.onopen = () => {
                console.log("Connected to WebSocket");
                statusEl.textContent = "Connected";
            };
            
            ws.onclose = () => {
                console.log("Disconnected");
                statusEl.textContent = "Disconnected (Reconnecting...)";
                setTimeout(connectWebSocket, 3000);
            };
            
            ws.onerror = (err) => {
                console.error("WebSocket Error:", err);
                statusEl.textContent = "Connection Error";
            };
            
            ws.onmessage = handleMessage;
        }

        connectWebSocket();
        
        // Message Handling
        async function handleMessage(event) {
            const data = JSON.parse(event.data);
            
            switch (data.type) {
                case "llm_token":
                    appendAssistantToken(data.text);
                    lastAgentResponse += data.text; // Accumulate for echo checking
                    break;
                case "audio":
                    if (currentState !== AppState.SPEAKING) {
                        transitionTo(AppState.SPEAKING);
                    }
                    playAudioChunk(data.data);
                    break;
                case "status":
                    statusEl.textContent = data.message;
                    break;
                case "response_complete":
                    // Handled when audio finishes
                    break;
                case "error":
                    alert("Error: " + data.message);
                    updateUIState(false);
                    break;
            }
        }

        // --- STT Logic (Web Speech API) ---

        function startRecording() {
            if (isRecording) return;
            
            ensureAudioContext();
            
            transitionTo(AppState.LISTENING);
        }

        function stopRecording() {
            if (!isRecording) return;
            transitionTo(AppState.IDLE);
        }

        function startMic() {
            if (isRecording) return;
            
            // Clean up old recognition if exists
            if (recognition) {
                try { recognition.abort(); } catch(e) {}
            }

            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            recognition = new SpeechRecognition();
            recognition.continuous = true; // Ambient Requirement
            recognition.interimResults = true;
            recognition.lang = 'en-US';

            recognition.onstart = () => {
                isRecording = true;
                // Don't update UI here, State Machine handles it
            };

            recognition.onend = () => {
                isRecording = false;
                // Auto-restart if in Ambient Mode and supposed to be LISTENING
                if (isAmbientMode && (currentState === AppState.LISTENING || currentState === AppState.SPEAKING)) {
                    console.log("Mic stopped, restarting for Ambient Mode...");
                    startMic(); 
                } else if (currentState === AppState.LISTENING && !isAmbientMode) {
                     // If manual mode, go back to IDLE
                     transitionTo(AppState.IDLE);
                }
            };

            recognition.onresult = (event) => {
                let interimTranscript = '';
                let finalTranscript = '';

                for (let i = event.resultIndex; i < event.results.length; ++i) {
                    if (event.results[i].isFinal) {
                        finalTranscript += event.results[i][0].transcript;
                    } else {
                        interimTranscript += event.results[i][0].transcript;
                    }
                }
                
                // 3. SPEAKING STATE INPUT BLOCK & 5. BARGE-IN
                if (currentState === AppState.SPEAKING) {
                    if (finalTranscript || interimTranscript) {
                         const detectedText = (finalTranscript + interimTranscript).trim();
                         
                         // 6. FAIL-SAFE / ECHO CHECK
                         // Simple heuristic: if it matches what we just said, ignore it.
                         // But for stricter safety: 
                         // Check if detected text is subset of lastAgentResponse
                         if (isEcho(detectedText)) {
                             console.log(`Ignored Echo: "${detectedText}"`);
                             return; // DROP INPUT
                         }

                         // If NOT echo -> IT IS USER INTERRUPTION
                         console.log(`Barge-in detected: "${detectedText}"`);
                         handleBargeIn(detectedText);
                         return;
                    }
                }

                if (currentState === AppState.LISTENING) {
                    if (interimTranscript) {
                        updatePartialUserMessage(interimTranscript);
                    }
                    if (finalTranscript) {
                        finalizeUserMessage(finalTranscript);
                        // Rule 4: LISTENING -> PROCESSING
                        transitionTo(AppState.PROCESSING);
                        
                        // Clear echo buffer for next turn
                        lastAgentResponse = ""; 
                        
                        sendText(finalTranscript);
                        // In One-Shot mode, we stop after one command.
                        // In Ambient, we keep going (state machine handles it).
                    }
                }
            };

            recognition.start();
        }

        function stopMic() {
            if (recognition) {
                recognition.stop();
                isRecording = false;
            }
        }

        function isEcho(text) {
            if (!text || !lastAgentResponse) return false;
            // Normalize
            const t1 = text.toLowerCase().replace(/[^\w\s]/g, '');
            const t2 = lastAgentResponse.toLowerCase().replace(/[^\w\s]/g, '');
            
            // If the input is contained in the output (or vice versa due to partials)
            // It's likely an echo.
            // Strict: If > 50% overlap?
            // Safer: If text is short (< 10 chars), might be noise.
            
            // Basic Containment Check
            if (t2.includes(t1) || t1.includes(t2)) return true;
            
            return false;
        }

        function handleBargeIn(text) {
            // 5. USER INTERRUPTION
            // Stop TTS
            stopAudioPlayback();
            
            // Send STOP to backend
            if (ws && ws.readyState === WebSocket.OPEN) {
                ws.send(JSON.stringify({ type: "stop" }));
            }
            
            // Transition back to LISTENING (or PROCESSING if we have final text)
            // If 'text' is final, we process it. If interim, we just start listening.
            // Actually, if we caught text, we should use it? 
            // Usually barge-in starts with "Stop" or "Hey".
            // Let's reset state to LISTENING.
            
            transitionTo(AppState.LISTENING);
            
            // If we have substantial text, treat as new input?
            // The logic above in onresult will catch 'finalTranscript' in the NEXT event loop 
            // if we are in LISTENING state? 
            // No, the event loop is here. We have 'text'.
            // If it's final, send it.
            // But usually barge-in is messy. 
            // Better to just stop everything and let user speak again?
            // Or if text is robust, use it.
            // User says: "Treat this as a new user turn"
            
            // Implementation:
            // 1. Stop everything.
            // 2. If 'text' is useful, send it.
            // But 'text' might be "Stop" or "Wait".
            // Let's just switch to LISTENING. The user will likely continue speaking or the current text is the command.
            
            // We need to verify if 'text' was final.
            // In the logic above, I passed 'detectedText'. I didn't pass isFinal.
            // Let's just switch to listening. The user is interrupting.
        }

        function sendText(text) {
            if (ws && ws.readyState === WebSocket.OPEN) {
                ws.send(JSON.stringify({ 
                    type: "query", 
                    text: text,
                    source: "user" // 2. HARD AUDIO OWNERSHIP RULE
                }));
            }
        }
        
        // --- Audio Playback Logic ---

        // Helper to stop audio
        function stopAudioPlayback() {
             // Stop Web Audio API sources
             activeSources.forEach(source => {
                 try { source.stop(); } catch(e) {}
             });
             activeSources = [];
             nextAudioTime = 0;
        }

        async function playAudioChunk(base64Data) {
            if (!audioContext) {
                console.warn("Audio received but AudioContext is missing. Attempting to restore...");
                ensureAudioContext();
                if (!audioContext) return;
            }
            
            try {
                // Decode Base64 to ArrayBuffer
                const binaryString = window.atob(base64Data);
                const len = binaryString.length;
                const bytes = new Uint8Array(len);
                for (let i = 0; i < len; i++) {
                    bytes[i] = binaryString.charCodeAt(i);
                }
                
                // Decode Audio Data (MP3/WAV -> PCM)
                const audioBuffer = await audioContext.decodeAudioData(bytes.buffer);
                
                const source = audioContext.createBufferSource();
                source.buffer = audioBuffer;
                source.connect(audioContext.destination);
                
                // Track source for barge-in
                source.onended = () => {
                    const index = activeSources.indexOf(source);
                    if (index > -1) activeSources.splice(index, 1);
                    
                    if (activeSources.length === 0) {
                        // Audio finished
                        if (currentState === AppState.SPEAKING) {
                            if (isAmbientMode) {
                                transitionTo(AppState.LISTENING);
                            } else {
                                transitionTo(AppState.IDLE);
                            }
                        }
                    }
                };
                activeSources.push(source);
                
                // Gapless Scheduling
                const currentTime = audioContext.currentTime;
                if (nextAudioTime < currentTime) {
                    nextAudioTime = currentTime + 0.05; // Small buffer if falling behind
                }
                
                source.start(nextAudioTime);
                nextAudioTime += audioBuffer.duration;
                
            } catch (err) {
                console.error("Error playing audio chunk:", err);
            }
        }

        // --- UI Logic ---

        function updateUIState(recording) {
            startBtn.disabled = recording;
            stopBtn.disabled = !recording;
        }
        
        let currentPartialDiv = null;
        let currentAssistantDiv = null;

        function updatePartialUserMessage(text) {
            if (!text.trim()) return;
            
            if (!currentPartialDiv) {
                currentPartialDiv = document.createElement('div');
                currentPartialDiv.className = 'message user-message partial';
                chatBox.appendChild(currentPartialDiv);
            }
            currentPartialDiv.textContent = text + "...";
            scrollToBottom();
        }

        function finalizeUserMessage(text) {
            if (currentPartialDiv) {
                currentPartialDiv.remove();
                currentPartialDiv = null;
            }
            
            if (text.trim()) {
                const msg = document.createElement('div');
                msg.className = 'message user-message';
                msg.textContent = text;
                chatBox.appendChild(msg);
                scrollToBottom();
            }
            
            // Prepare for assistant response
            currentAssistantDiv = document.createElement('div');
            currentAssistantDiv.className = 'message assistant-message';
            chatBox.appendChild(currentAssistantDiv);
        }

        function appendAssistantToken(token) {
            if (!currentAssistantDiv) {
                currentAssistantDiv = document.createElement('div');
                currentAssistantDiv.className = 'message assistant-message';
                chatBox.appendChild(currentAssistantDiv);
            }
            currentAssistantDiv.textContent += token;
            scrollToBottom();
        }

        function scrollToBottom() {
            chatBox.scrollTop = chatBox.scrollHeight;
        }
    </script>
</body>
</html>
