<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Assistant</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <style>
        :root {
            --primary-color: #007aff;
            --bg-color: #121212;
            --surface-color: #1e1e1e;
            --text-color: #ffffff;
            --secondary-text: #a0a0a0;
            --user-msg-bg: #2c2c2e;
            --ai-msg-bg: #007aff;
            --orb-idle: #444;
            --orb-listening: #00ff88;
            --orb-speaking: #007aff;
            --orb-processing: #bf5af2;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            background-color: var(--bg-color);
            color: var(--text-color);
            margin: 0;
            height: 100vh;
            display: flex;
            flex-direction: column;
            overflow: hidden;
        }

        /* Main Visualizer Area */
        .visualizer-container {
            flex: 1;
            display: flex;
            justify-content: center;
            align-items: center;
            position: relative;
            min-height: 300px;
        }

        .orb {
            width: 120px;
            height: 120px;
            border-radius: 50%;
            background: var(--orb-idle);
            box-shadow: 0 0 20px rgba(255, 255, 255, 0.1);
            transition: all 0.5s ease;
            position: relative;
        }

        /* Animation States */
        .orb.listening {
            background: var(--orb-listening);
            box-shadow: 0 0 40px var(--orb-listening);
            animation: pulse-listening 1.5s infinite;
        }

        .orb.speaking {
            background: var(--orb-speaking);
            box-shadow: 0 0 40px var(--orb-speaking);
            animation: wave-speaking 1s infinite;
        }

        .orb.processing {
            background: var(--orb-processing);
            box-shadow: 0 0 30px var(--orb-processing);
            animation: spin-processing 2s linear infinite;
        }

        @keyframes pulse-listening {
            0% { transform: scale(1); opacity: 1; }
            50% { transform: scale(1.1); opacity: 0.8; }
            100% { transform: scale(1); opacity: 1; }
        }

        @keyframes wave-speaking {
            0% { transform: scale(1); border-radius: 50%; }
            25% { transform: scale(1.05, 0.95); border-radius: 45%; }
            50% { transform: scale(1); border-radius: 50%; }
            75% { transform: scale(0.95, 1.05); border-radius: 45%; }
            100% { transform: scale(1); border-radius: 50%; }
        }

        @keyframes spin-processing {
            0% { transform: rotate(0deg) scale(1); border-radius: 40%; }
            50% { transform: rotate(180deg) scale(0.9); border-radius: 50%; }
            100% { transform: rotate(360deg) scale(1); border-radius: 40%; }
        }

        /* Chat Overlay */
        .chat-overlay {
            height: 35vh;
            padding: 20px;
            overflow-y: auto;
            display: flex;
            flex-direction: column;
            gap: 12px;
            mask-image: linear-gradient(to bottom, transparent, black 10%);
        }

        .message {
            max-width: 85%;
            padding: 12px 18px;
            border-radius: 20px;
            font-size: 16px;
            line-height: 1.4;
            animation: fadeIn 0.3s ease;
        }

        .user-message {
            align-self: flex-end;
            background-color: var(--user-msg-bg);
            color: var(--text-color);
            border-bottom-right-radius: 4px;
        }

        .assistant-message {
            align-self: flex-start;
            background-color: var(--ai-msg-bg);
            color: white;
            border-bottom-left-radius: 4px;
        }

        .partial {
            opacity: 0.7;
            font-style: italic;
        }

        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(10px); }
            to { opacity: 1; transform: translateY(0); }
        }

        /* Controls Bar */
        .controls-bar {
            padding: 20px;
            display: flex;
            justify-content: center;
            align-items: center;
            gap: 30px;
            background: rgba(30, 30, 30, 0.8);
            backdrop-filter: blur(10px);
            border-top: 1px solid #333;
        }

        .icon-btn {
            background: none;
            border: none;
            color: var(--secondary-text);
            font-size: 24px;
            cursor: pointer;
            padding: 10px;
            border-radius: 50%;
            transition: all 0.2s;
        }

        .icon-btn:hover {
            color: white;
            background: rgba(255, 255, 255, 0.1);
        }

        .icon-btn.active {
            color: var(--primary-color);
        }
        
        .icon-btn.muted {
            color: #ff3b30;
        }

        .main-btn {
            width: 70px;
            height: 70px;
            border-radius: 50%;
            border: none;
            background: var(--primary-color);
            color: white;
            font-size: 28px;
            cursor: pointer;
            box-shadow: 0 4px 15px rgba(0, 122, 255, 0.3);
            transition: transform 0.2s, background 0.2s;
            display: flex;
            align-items: center;
            justify-content: center;
        }

        .main-btn:active {
            transform: scale(0.95);
        }

        .main-btn.stop {
            background: #ff3b30;
            box-shadow: 0 4px 15px rgba(255, 59, 48, 0.3);
        }

        /* Settings Panel */
        .settings-panel {
            position: absolute;
            bottom: 100px;
            right: 20px;
            background: var(--surface-color);
            padding: 20px;
            border-radius: 16px;
            box-shadow: 0 10px 40px rgba(0,0,0,0.5);
            width: 280px;
            transform: translateY(20px);
            opacity: 0;
            pointer-events: none;
            transition: all 0.3s ease;
            z-index: 100;
        }

        .settings-panel.visible {
            transform: translateY(0);
            opacity: 1;
            pointer-events: auto;
        }

        .settings-row {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 15px;
        }
        
        .settings-row:last-child { margin-bottom: 0; }

        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 28px;
        }
        .switch input { opacity: 0; width: 0; height: 0; }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0; left: 0; right: 0; bottom: 0;
            background-color: #444;
            transition: .4s;
            border-radius: 34px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 20px;
            width: 20px;
            left: 4px;
            bottom: 4px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider { background-color: var(--primary-color); }
        input:checked + .slider:before { transform: translateX(22px); }

        select {
            background: #333;
            color: white;
            border: none;
            padding: 8px 12px;
            border-radius: 8px;
            outline: none;
        }

        .status-text {
            position: absolute;
            top: 20px;
            left: 50%;
            transform: translateX(-50%);
            color: var(--secondary-text);
            font-size: 14px;
            background: rgba(0,0,0,0.3);
            padding: 5px 15px;
            border-radius: 20px;
        }

    </style>
</head>
<body>
    <div class="container">
        <h1>Voice Assistant</h1>
        
        <div class="status" id="status">Ready</div>
        
        <div class="controls">
            <button id="startBtn" onclick="startRecording()">Start Recording</button>
            <button id="stopBtn" onclick="stopRecording()" disabled>Stop Recording</button>
            
            <select id="languageSelect" class="language-select" onchange="handleLanguageChange()">
                <option value="en-US">English (US)</option>
            </select>

            <div class="switch-container">
                <span class="switch-label">Ambient Mode</span>
                <label class="switch">
                    <input type="checkbox" id="ambientToggle" onchange="toggleAmbientMode()">
                    <span class="slider"></span>
                </label>
            </div>
        </div>

    <div class="status-text" id="status">Ready</div>

    <div class="visualizer-container">
        <div class="orb" id="orb"></div>
    </div>

    <div class="chat-overlay" id="chatBox">
        <!-- Messages will appear here -->
    </div>

    <div class="controls-bar">
        <button id="muteBtn" class="icon-btn" onclick="toggleMute()" title="Mute Microphone">
            <i class="fas fa-microphone-slash"></i>
        </button>

        <button id="mainBtn" class="main-btn" onclick="handleMainAction()">
            <i class="fas fa-microphone"></i>
        </button>

        <button id="settingsBtn" class="icon-btn" onclick="toggleSettings()" title="Settings">
            <i class="fas fa-cog"></i>
        </button>
    </div>

    <!-- Hidden Settings Panel -->
    <div class="settings-panel" id="settingsPanel">
        <div class="settings-row">
            <span>Ambient Mode</span>
            <label class="switch">
                <input type="checkbox" id="ambientToggle" onchange="toggleAmbientMode()" checked>
                <span class="slider"></span>
            </label>
        </div>
        <div class="settings-row">
            <span>Language</span>
            <select id="languageSelect" onchange="handleLanguageChange()">
                <option value="en-US">English (US)</option>
            </select>
        </div>
    </div>

    <script>
        // --- Configuration ---
        const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
        const host = window.location.host;
        // Use dynamic host if served from backend, otherwise fallback to localhost:8000
        const WS_URL = host ? `${protocol}//${host}/ws` : "ws://localhost:8000/ws";
        const MAX_MESSAGES = 4;

        // --- Global State ---
        let ws = null;
        let audioContext = null;
        let recognition = null;
        let isRecording = false; // "Mic Active" flag (HW level)
        let isMuted = false;

        const AppState = {
            IDLE: 'idle',
            LISTENING: 'listening',
            PROCESSING: 'processing',
            SPEAKING: 'speaking'
        };
        let currentState = AppState.IDLE;
        let isAmbientMode = true;

        // Echo Gating
        let lastAgentResponse = ""; 
        let ignoreAudioUntil = 0;

        // Audio Playback
        let activeSources = [];
        let nextStartTime = 0; // Track when the next chunk should play

        // --- UI Elements ---
        const statusEl = document.getElementById('status');
        const orb = document.getElementById('orb');
        const chatBox = document.getElementById('chatBox');
        const mainBtn = document.getElementById('mainBtn');
        const muteBtn = document.getElementById('muteBtn');
        const settingsPanel = document.getElementById('settingsPanel');
        const ambientToggle = document.getElementById('ambientToggle');
        const languageSelect = document.getElementById('languageSelect');

        // --- Initialization ---
        function ensureAudioContext() {
            if (!audioContext) {
                 const AudioContext = window.AudioContext || window.webkitAudioContext;
                 if (!AudioContext) {
                     alert("Web Audio API is not supported in this browser");
                     return;
                 }
                 audioContext = new AudioContext({ sampleRate: 24000 });
            }
            if (audioContext.state === 'suspended') {
                audioContext.resume();
            }
        }

        // UI Elements
        const statusEl = document.getElementById('status');
        const chatBox = document.getElementById('chatBox');
        const startBtn = document.getElementById('startBtn');
        const stopBtn = document.getElementById('stopBtn');
        const ambientToggle = document.getElementById('ambientToggle');
        const languageSelect = document.getElementById('languageSelect');

        // Language Management
        async function loadLanguages() {
            try {
                const response = await fetch('http://localhost:8000/languages');
                const languages = await response.json();
                
                languageSelect.innerHTML = '';
                let defaultLang = 'en-US';
                
                // Prioritize en-IN for Hinglish users if available, else en-US
                if (languages['en-IN']) defaultLang = 'en-IN';
                
                for (const [key, lang] of Object.entries(languages)) {
                    const option = document.createElement('option');
                    option.value = lang.code;
                    option.textContent = lang.name;
                    if (lang.code === defaultLang) option.selected = true;
                    languageSelect.appendChild(option);
                }
            } catch (e) {
                console.error("Failed to load languages:", e);
            }
        }

        function handleLanguageChange() {
            console.log("Language changed to:", languageSelect.value);
            // If currently recording, restart to apply new language
            if (isRecording) {
                stopMic();
                startMic();
            }
        }

        loadLanguages();

        // State Management
        function transitionTo(newState) {
            console.log(`State Transition: ${currentState} -> ${newState}`);
            currentState = newState;
            
            // Reset audio scheduler on state change (especially to IDLE/LISTENING)
            if (newState === AppState.IDLE || newState === AppState.LISTENING) {
                nextStartTime = 0;
            }

            // Remove all animation classes
            orb.classList.remove('listening', 'speaking', 'processing');

            // UI Updates
            switch (newState) {
                case AppState.IDLE:
                    statusEl.textContent = "Tap mic to start";
                    updateMainButton('start');
                    stopMic();
                    break;
                    
                case AppState.LISTENING:
                    statusEl.textContent = "Listening...";
                    orb.classList.add('listening');
                    updateMainButton('stop');
                    startMic();
                    break;
                    
                case AppState.PROCESSING:
                    statusEl.textContent = "Thinking...";
                    orb.classList.add('processing');
                    // In processing, we generally keep the state as is regarding the mic
                    // unless we want to lock it. For now, visual only.
                    break;
                    
                case AppState.SPEAKING:
                    statusEl.textContent = "Speaking...";
                    orb.classList.add('speaking');
                    // Start heavy gating
                    ignoreAudioUntil = Date.now() + 60000; // Ignore all input while speaking
                    break;
            }
        }

        function updateMainButton(type) {
            if (type === 'start') {
                mainBtn.innerHTML = '<i class="fas fa-microphone"></i>';
                mainBtn.classList.remove('stop');
            } else {
                mainBtn.innerHTML = '<i class="fas fa-stop"></i>';
                mainBtn.classList.add('stop');
            }
        }

        function handleMainAction() {
            ensureAudioContext();
            
            if (currentState === AppState.IDLE) {
                transitionTo(AppState.LISTENING);
            } else {
                // If in any other state, "Stop" button basically resets to IDLE
                // Or if in LISTENING, it stops.
                transitionTo(AppState.IDLE);
            }
        }

        function toggleMute() {
            isMuted = !isMuted;
            muteBtn.classList.toggle('muted', isMuted);
            const icon = muteBtn.querySelector('i');
            icon.className = isMuted ? 'fas fa-microphone-slash' : 'fas fa-microphone';
            
            if (isMuted) {
                stopMic();
                statusEl.textContent = "Muted";
            } else {
                // If unmuting, resume based on state
                if (currentState === AppState.LISTENING || isAmbientMode) {
                    startMic();
                    statusEl.textContent = "Listening...";
                } else {
                    statusEl.textContent = "Ready";
                }
            }
        }

        function toggleSettings() {
            settingsPanel.classList.toggle('visible');
            const btn = document.getElementById('settingsBtn');
            btn.classList.toggle('active');
        }

        function toggleAmbientMode() {
            ensureAudioContext();
            isAmbientMode = ambientToggle.checked;
            
            if (isAmbientMode) {
                if (currentState === AppState.IDLE) {
                    transitionTo(AppState.LISTENING);
                }
            } else {
                if (currentState === AppState.LISTENING) {
                    transitionTo(AppState.IDLE);
                }
                if (ws && ws.readyState === WebSocket.OPEN) {
                    ws.send(JSON.stringify({ type: "reset_context" }));
                }
            }
        }

        // --- Message Handling ---
        function addMessage(text, type) {
            const div = document.createElement('div');
            div.className = `message ${type}-message`;
            div.textContent = text;
            chatBox.appendChild(div);
            
            // Limit history
            while (chatBox.children.length > MAX_MESSAGES) {
                chatBox.removeChild(chatBox.firstChild);
            }
            
            chatBox.scrollTop = chatBox.scrollHeight;
            return div;
        }

        let currentAssistantMessageDiv = null;

        function appendAssistantToken(text) {
            if (!currentAssistantMessageDiv) {
                currentAssistantMessageDiv = addMessage("", "assistant");
            }
            currentAssistantMessageDiv.textContent += text;
            chatBox.scrollTop = chatBox.scrollHeight;
        }
        
        // --- STT & Audio Logic (Ported & Adapted) ---
        
        async function loadLanguages() {
            try {
                // Use relative URL to fetch languages
                const response = await fetch('/languages');
                const languages = await response.json();
                languageSelect.innerHTML = '';
                let defaultLang = 'en-US';
                if (languages['en-IN']) defaultLang = 'en-IN';
                for (const [key, lang] of Object.entries(languages)) {
                    const option = document.createElement('option');
                    option.value = lang.code;
                    option.textContent = lang.name;
                    if (lang.code === defaultLang) option.selected = true;
                    languageSelect.appendChild(option);
                }
            } catch (e) { console.error("Lang load failed", e); }
        }
        loadLanguages();

        function handleLanguageChange() {
            if (isRecording) {
                stopMic();
                startMic();
            }
        }

        function createRecognition() {
            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            recognition = new SpeechRecognition();
            recognition.continuous = true; // Ambient Requirement
            recognition.interimResults = true;
            recognition.lang = languageSelect.value || 'en-US';

            rec.onstart = () => {
                isRecording = true;
            };

            rec.onend = () => {
                isRecording = false;

                // If Speaking, we expect the mic to be off (Echo Cancellation).
                // Restarting will happen via audio playback end event.
                if (currentState === AppState.SPEAKING) return;

                if (!isMuted && isAmbientMode && (currentState === AppState.LISTENING || currentState === AppState.PROCESSING)) {
                    // Slight delay to prevent tight loops if errors occur
                    setTimeout(() => {
                        if (currentState === AppState.LISTENING || currentState === AppState.PROCESSING) {
                             startMic();
                        }
                    }, 100);
                } else if (!isMuted && currentState === AppState.LISTENING && !isAmbientMode) {
                     transitionTo(AppState.IDLE);
                }
            };

            rec.onresult = (event) => {
                let interimTranscript = '';
                let finalTranscript = '';

                for (let i = event.resultIndex; i < event.results.length; ++i) {
                    if (event.results[i].isFinal) {
                        finalTranscript += event.results[i][0].transcript;
                    } else {
                        interimTranscript += event.results[i][0].transcript;
                    }
                }
                
                // Prevent self-hearing (Echo Cancellation Strategy: Mute STT during TTS)
                // We ignore all inputs while the AI is speaking to prevent feedback loops.
                if (currentState === AppState.SPEAKING) {
                    return;
                }
                
                // Cool-down check
                if (Date.now() < ignoreAudioUntil) {
                    console.log("Ignored audio due to cool-down (echo cancellation)");
                    return;
                }

                if (currentState === AppState.LISTENING) {
                    if (interimTranscript) {
                        statusEl.textContent = `Listening: "${interimTranscript}"`;
                    }
                    
                    if (finalTranscript) {
                        addMessage(finalTranscript, "user");
                        transitionTo(AppState.PROCESSING);
                        lastAgentResponse = ""; 
                        currentAssistantMessageDiv = null; 
                        sendText(finalTranscript);
                    }
                }
            };
            
            return rec;
        }

        function startMic() {
            if (isRecording || isMuted) return;
            
            if (!recognition) {
                recognition = createRecognition();
            } else {
                // Update language if needed
                if (recognition.lang !== languageSelect.value) {
                    recognition.lang = languageSelect.value || 'en-US';
                }
            }

            try {
                recognition.start();
            } catch (e) {
                // Already started or other error
                console.log("Recognition start error:", e);
            }
        }

        function stopMic() {
            if (recognition) {
                recognition.stop();
                isRecording = false;
            }
        }


        function isEcho(text) {
            if (!text || !lastAgentResponse) return false;
            const t1 = text.toLowerCase().replace(/[^\w\s]/g, '');
            const t2 = lastAgentResponse.toLowerCase().replace(/[^\w\s]/g, '');
            if (t2.includes(t1) || t1.includes(t2)) return true;
            return false;
        }

        function handleBargeIn(text) {
            stopAudioPlayback();
            if (ws && ws.readyState === WebSocket.OPEN) {
                ws.send(JSON.stringify({ type: "stop" }));
            }
            transitionTo(AppState.LISTENING);
        }

        function sendText(text) {
            if (ws && ws.readyState === WebSocket.OPEN) {
                ws.send(JSON.stringify({ 
                    type: "query", 
                    text: text,
                    language: languageSelect.value || 'en-US',
                    source: "user" // 2. HARD AUDIO OWNERSHIP RULE
                }));
            }
        }

        // --- WebSocket ---
        function connectWebSocket() {
            ws = new WebSocket(WS_URL);
            ws.onopen = () => { statusEl.textContent = "Tap mic to start"; };
            ws.onclose = () => { 
                statusEl.textContent = "Disconnected"; 
                setTimeout(connectWebSocket, 3000); 
            };
            ws.onerror = (err) => { statusEl.textContent = "Connection Error"; };
            ws.onmessage = handleMessage;
        }
        connectWebSocket();

        async function handleMessage(event) {
            const data = JSON.parse(event.data);
            switch (data.type) {
                case "llm_token":
                    appendAssistantToken(data.text);
                    lastAgentResponse += data.text; 
                    break;
                case "audio":
                    if (currentState !== AppState.SPEAKING) {
                        transitionTo(AppState.SPEAKING);
                    }
                    playAudioChunk(data.data);
                    break;
                case "status":
                    // Only show important statuses or if debugging
                    // statusEl.textContent = data.message;
                    break;
                case "error":
                    alert(data.message);
                    transitionTo(AppState.IDLE);
                    break;
            }
        }

        // --- Audio Playback ---
        function stopAudioPlayback() {
             activeSources.forEach(source => {
                 try { source.stop(); } catch(e) {}
             });
             activeSources = [];
        }

        async function playAudioChunk(base64Data) {
            if (!audioContext) return;
            try {
                const binaryString = window.atob(base64Data);
                const len = binaryString.length;
                const bytes = new Uint8Array(len);
                for (let i = 0; i < len; i++) bytes[i] = binaryString.charCodeAt(i);
                
                const audioBuffer = await audioContext.decodeAudioData(bytes.buffer);
                const source = audioContext.createBufferSource();
                source.buffer = audioBuffer;
                source.connect(audioContext.destination);
                
                source.onended = () => {
                    activeSources = activeSources.filter(s => s !== source);
                    if (activeSources.length === 0 && currentState === AppState.SPEAKING) {
                         // Check if we are truly done (no pending audio in schedule)
                         // But for now, we rely on the backend sending "response_complete" or similar, 
                         // OR just timeout. 
                         // Actually, with streaming, we might have gaps.
                         // Let's rely on the fact that if this ends, and nextStartTime is close to now, we are done?
                         
                         // Re-implement end logic:
                         // We don't auto-transition here because more chunks might be coming.
                         // We should wait for a specific "Done" signal or a timeout.
                         
                         // However, keeping existing logic for now but adding a small check
                         if (audioContext.currentTime >= nextStartTime - 0.1) {
                             // Buffer is empty
                             ignoreAudioUntil = Date.now() + 2000; 
                             if (isAmbientMode) transitionTo(AppState.LISTENING);
                             else transitionTo(AppState.IDLE);
                         }
                    }
                };
                
                // Schedule playback
                const now = audioContext.currentTime;
                // If nextStartTime is in the past (gap), start now.
                // If it's in the future, wait until then.
                if (nextStartTime < now) nextStartTime = now;
                
                source.start(nextStartTime);
                
                // Advance nextStartTime
                // Reduce gap between sentences by overlapping slightly (e.g., 50ms)
                // This makes the conversation flow more naturally.
                nextStartTime += Math.max(0, audioBuffer.duration - 0.05);
                
                activeSources.push(source);
            } catch (e) { console.error("Audio Playback Error", e); }
        }

    </script>
</body>
</html>